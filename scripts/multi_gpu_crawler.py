#!/usr/bin/env python3
"""
Crawler optimizat pentru multiple GPU-uri
"""

import torch
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel
import os
import asyncio
from concurrent.futures import ProcessPoolExecutor
import logging
from improved_crawler import YouTubeCrawler, Config

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MultiGPUCrawler:
    def __init__(self, config: Config):
        self.config = config
        self.gpu_count = torch.cuda.device_count()
        logger.info(f"üéÆ Detectate {self.gpu_count} GPU-uri")
        
    def setup_gpu_worker(self, gpu_id, video_batch):
        """ConfigureazƒÉ un worker pentru un GPU specific"""
        torch.cuda.set_device(gpu_id)
        device = torch.device(f'cuda:{gpu_id}')
        
        logger.info(f"üöÄ Worker GPU {gpu_id} proceseazƒÉ {len(video_batch)} videoclipuri")
        
        # CreeazƒÉ crawler pentru acest GPU
        crawler = YouTubeCrawler(self.config)
        
        # ProceseazƒÉ batch-ul de videoclipuri
        results = []
        for video_data in video_batch:
            try:
                # Aici ai putea adƒÉuga procesarea specificƒÉ GPU
                result = asyncio.run(crawler.process_video_async(video_data))
                results.append(result)
            except Exception as e:
                logger.error(f"Eroare procesare video pe GPU {gpu_id}: {e}")
        
        return results
    
    def distribute_videos(self, videos):
        """Distribuie videoclipurile pe GPU-uri"""
        if self.gpu_count == 0:
            logger.warning("Nu sunt GPU-uri disponibile, folosesc CPU")
            return [videos]
        
        batch_size = len(videos) // self.gpu_count
        batches = []
        
        for i in range(self.gpu_count):
            start_idx = i * batch_size
            if i == self.gpu_count - 1:  # Ultimul batch ia restul
                end_idx = len(videos)
            else:
                end_idx = (i + 1) * batch_size
            
            batch = videos[start_idx:end_idx]
            if batch:  # Doar dacƒÉ batch-ul nu e gol
                batches.append(batch)
        
        return batches
    
    async def crawl_with_multiple_gpus(self, query, max_results=None):
        """Crawling cu multiple GPU-uri"""
        logger.info(f"üîç √éncep crawling cu {self.gpu_count} GPU-uri pentru: {query}")
        
        # Ob»õine lista de videoclipuri
        crawler = YouTubeCrawler(self.config)
        
        # Simulez ob»õinerea videoclipurilor (√Æn realitate ar veni din YouTube API)
        videos = []  # Aici ar fi lista realƒÉ de videoclipuri
        
        if not videos:
            logger.warning("Nu s-au gƒÉsit videoclipuri pentru procesare")
            return
        
        # Distribuie videoclipurile pe GPU-uri
        video_batches = self.distribute_videos(videos)
        
        if self.gpu_count > 0:
            # Procesare paralelƒÉ pe multiple GPU-uri
            with ProcessPoolExecutor(max_workers=self.gpu_count) as executor:
                futures = []
                for gpu_id, batch in enumerate(video_batches):
                    future = executor.submit(self.setup_gpu_worker, gpu_id, batch)
                    futures.append(future)
                
                # A»ôteaptƒÉ rezultatele
                results = []
                for future in futures:
                    try:
                        result = future.result(timeout=3600)  # 1 orƒÉ timeout
                        results.extend(result)
                    except Exception as e:
                        logger.error(f"Eroare √Æn procesarea GPU: {e}")
        else:
            # Fallback la CPU
            logger.info("Folosesc CPU pentru procesare")
            crawler = YouTubeCrawler(self.config)
            for video_data in videos:
                await crawler.process_video_async(video_data)

def optimize_gpu_memory():
    """OptimizeazƒÉ utilizarea memoriei GPU"""
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            torch.cuda.set_device(i)
            torch.cuda.empty_cache()
            
            # SeteazƒÉ memory fraction pentru a evita out-of-memory
            torch.cuda.set_per_process_memory_fraction(0.8, device=i)
            
        logger.info(f"‚úÖ Optimizat memoria pentru {torch.cuda.device_count()} GPU-uri")

async def main():
    """Func»õia principalƒÉ pentru crawling multi-GPU"""
    # OptimizeazƒÉ memoria GPU
    optimize_gpu_memory()
    
    # Configurare
    config = Config()
    config.MAX_WORKERS = torch.cuda.device_count() * 2  # 2 workers per GPU
    
    # CreeazƒÉ crawler multi-GPU
    multi_crawler = MultiGPUCrawler(config)
    
    # Queries pentru cƒÉutare
    queries = [
        "reclamƒÉ 2025 OR advertisement 2025 OR ad 2025 OR commercial 2025",
        "publicitate Rom√¢nia 2025",
        "marketing campaign 2025"
    ]
    
    # RuleazƒÉ crawling-ul pentru fiecare query
    for query in queries:
        try:
            await multi_crawler.crawl_with_multiple_gpus(query, max_results=200)
            logger.info(f"‚úÖ Completat crawling pentru query: {query}")
        except Exception as e:
            logger.error(f"‚ùå E»ôuat crawling pentru query '{query}': {e}")

if __name__ == "__main__":
    # SeteazƒÉ variabile de mediu pentru PyTorch
    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
    os.environ['TORCH_USE_CUDA_DSA'] = '1'
    
    # RuleazƒÉ crawling-ul
    asyncio.run(main())
